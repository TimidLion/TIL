## gradient vanishing problem

If one uses RNN, there is problem that RNN can not handle some long-range dependency things.

This happens because the formula of RNN includes some chain rule, and its gradient converges to 0.

For detail,

https://aikorea.org/blog/rnn-tutorial-3/

https://brunch.co.kr/@chris-song/39
